{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Cart_Pole_Custom.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **SYDE 552 Notebook**"],"metadata":{"id":"pdTrCEWuf-jw"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd 'gdrive'/'MyDrive'/'4th Year'/'SYDE 552'/'SYDE 552 Project'"],"metadata":{"id":"04TX5Wh_uGog","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650929859081,"user_tz":240,"elapsed":21040,"user":{"displayName":"Aleksander Ficek","userId":"04291360368766181612"}},"outputId":"f3d6c908-72a4-4d03-dfce-ead38001e93d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/4th Year/SYDE 552/SYDE 552 Project\n"]}]},{"cell_type":"code","source":["!apt-get install -qq ffmpeg freeglut3-dev xvfb  # For visualization\n","\n","!pip install -q git+https://github.com/DLR-RM/stable-baselines3#egg=stable-baselines3[extra]"],"metadata":{"id":"zJzo61NDhiZC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gym\n","import numpy as np\n","import torch as th\n","import matplotlib.pyplot as plt\n","\n","from stable_baselines3 import DQN, A2C, PPO\n","from stable_baselines3.common.evaluation import evaluate_policy"],"metadata":{"id":"tX1Ft5JVhjA4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gym\n","import numpy as np\n","import torch as th\n","import matplotlib.pyplot as plt\n","\n","from stable_baselines3 import A2C, DQN, PPO\n","from stable_baselines3.common.evaluation import evaluate_policy\n","from stable_baselines3.common.vec_env import VecFrameStack\n","from stable_baselines3.common.env_util import make_atari_env\n","\n","from stable_baselines3.common import results_plotter\n","from stable_baselines3.common.monitor import Monitor\n","from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n","\n","tensorboard_log = \"data_temp/tb/\"\n","env = gym.make(\"CartPole-v1\")\n","\n","model = A2C(\"MlpPolicy\",\n","            env,\n","            verbose=0,\n","            learning_rate=4e-3,\n","            tensorboard_log=tensorboard_log,\n","            seed=2)\n","\n","timesteps = int(3e5)\n","model.learn(timesteps, log_interval=10)\n","\n","mean_reward, std_reward = evaluate_policy(model, model.get_env(), deterministic=True, n_eval_episodes=20)\n","print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"],"metadata":{"id":"QK69LDyaFgQg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","import gym\n","from gym import spaces, logger\n","from gym.utils import seeding\n","import numpy as np\n","\n","class CartPoleEnv(gym.Env):\n","    metadata = {\n","        'render.modes': ['human', 'rgb_array'],\n","        'video.frames_per_second' : 50\n","    }\n","\n","    def __init__(self):\n","        self.gravity = 9.8\n","        self.masscart = 1.0\n","        self.masspole = 0.1\n","        self.total_mass = (self.masspole + self.masscart)\n","        self.length = 0.5 # actually half the pole's length\n","        self.polemass_length = (self.masspole * self.length)\n","        self.force_mag = 10.0\n","        self.tau = 0.02  # seconds between state updates\n","\n","        # Angle at which to fail the episode\n","        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n","        self.x_threshold = 2.4\n","\n","        # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds\n","        high = np.array([\n","            self.x_threshold * 2,\n","            np.finfo(np.float32).max,\n","            self.theta_threshold_radians * 2,\n","            np.finfo(np.float32).max])\n","\n","        self.action_space = spaces.Discrete(2)\n","        self.observation_space = spaces.Box(-high, high)\n","\n","        self.seed()\n","        self.viewer = None\n","        self.state = None\n","\n","        self.steps_beyond_done = None\n","\n","    def seed(self, seed=None):\n","        self.np_random, seed = seeding.np_random(seed)\n","        return [seed]\n","\n","    def step(self, action):\n","        assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n","        state = self.state\n","        x, x_dot, theta, theta_dot = state\n","        force = self.force_mag if action==1 else -self.force_mag\n","        costheta = math.cos(theta)\n","        sintheta = math.sin(theta)\n","        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass\n","        thetaacc = (self.gravity * sintheta - costheta* temp) / (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass))\n","        xacc  = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n","        x  = x + self.tau * x_dot\n","        x_dot = x_dot + self.tau * xacc\n","        theta = theta + self.tau * theta_dot\n","        theta_dot = theta_dot + self.tau * thetaacc\n","        self.state = (x,x_dot,theta,theta_dot)\n","        done =  x < -self.x_threshold \\\n","                or x > self.x_threshold \\\n","                or theta < -self.theta_threshold_radians \\\n","                or theta > self.theta_threshold_radians\n","        done = bool(done)\n","\n","        if not done:\n","            reward = 1.0\n","        elif self.steps_beyond_done is None:\n","            # Pole just fell!\n","            self.steps_beyond_done = 0\n","            reward = 1.0\n","        else:\n","            if self.steps_beyond_done == 0:\n","                logger.warn(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n","            self.steps_beyond_done += 1\n","            reward = 0.0\n","\n","        return np.array(self.state), reward, done, {}\n","\n","    def reset(self):\n","        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n","        self.steps_beyond_done = None\n","        return np.array(self.state)\n","\n","    def render(self, mode='human'):\n","        screen_width = 600\n","        screen_height = 400\n","\n","        world_width = self.x_threshold*2\n","        scale = screen_width/world_width\n","        carty = 100 # TOP OF CART\n","        polewidth = 10.0\n","        polelen = scale * 1.0\n","        cartwidth = 50.0\n","        cartheight = 30.0\n","\n","        if self.viewer is None:\n","            from gym.envs.classic_control import rendering\n","            self.viewer = rendering.Viewer(screen_width, screen_height)\n","            l,r,t,b = -cartwidth/2, cartwidth/2, cartheight/2, -cartheight/2\n","            axleoffset =cartheight/4.0\n","            cart = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n","            self.carttrans = rendering.Transform()\n","            cart.add_attr(self.carttrans)\n","            self.viewer.add_geom(cart)\n","            l,r,t,b = -polewidth/2,polewidth/2,polelen-polewidth/2,-polewidth/2\n","            pole = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n","            pole.set_color(.8,.6,.4)\n","            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n","            pole.add_attr(self.poletrans)\n","            pole.add_attr(self.carttrans)\n","            self.viewer.add_geom(pole)\n","            self.axle = rendering.make_circle(polewidth/2)\n","            self.axle.add_attr(self.poletrans)\n","            self.axle.add_attr(self.carttrans)\n","            self.axle.set_color(.5,.5,.8)\n","            self.viewer.add_geom(self.axle)\n","            self.track = rendering.Line((0,carty), (screen_width,carty))\n","            self.track.set_color(0,0,0)\n","            self.viewer.add_geom(self.track)\n","\n","        if self.state is None: return None\n","\n","        x = self.state\n","        cartx = x[0]*scale+screen_width/2.0 # MIDDLE OF CART\n","        self.carttrans.set_translation(cartx, carty)\n","        self.poletrans.set_rotation(-x[2])\n","\n","        return self.viewer.render(return_rgb_array = mode=='rgb_array')\n","\n","    def close(self):\n","        if self.viewer: self.viewer.close()"],"metadata":{"id":"eVgbXux9hH1F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# env = CartPoleEnv()\n","# env = gym.make(\"custom-cart-pole-v0\")\n","env = gym.make(\"CartPole-v1\")\n","# env = gym.make(\"LunarLanderContinuous-v2\")\n","# env = gym.make(\"BreakoutNoFrameskip-v4\")"],"metadata":{"id":"gExNfeojiHM3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tensorboard_log = \"data_cart_custom/tb/\""],"metadata":{"id":"CZrNIIKFiJcx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model = DQN(\"MlpPolicy\",\n","#             env,\n","#             verbose=0,\n","#             train_freq=16,\n","#             gradient_steps=8,\n","#             gamma=0.99,\n","#             exploration_fraction=0.2,\n","#             exploration_final_eps=0.07,\n","#             target_update_interval=600,\n","#             learning_starts=1000,\n","#             buffer_size=10000,\n","#             batch_size=128,\n","#             learning_rate=4e-3,\n","#             policy_kwargs=dict(net_arch=[256, 256]),\n","#             tensorboard_log=tensorboard_log,\n","#             seed=2)\n","\n","model = A2C(\"MlpPolicy\",\n","            env,\n","            verbose=0,\n","            learning_rate=4e-3,\n","            tensorboard_log=tensorboard_log,\n","            seed=2)\n","\n","# model = PPO(\"MlpPolicy\",\n","#             env,\n","#             verbose=1,\n","#             learning_rate=4e-3,\n","#             tensorboard_log=tensorboard_log,\n","#             seed=2)"],"metadata":{"id":"CJr9Vx6liLBy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mean_reward, std_reward = evaluate_policy(model, model.get_env(), deterministic=True, n_eval_episodes=20)\n","\n","print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hezv8uwwiObL","executionInfo":{"status":"ok","timestamp":1650929406664,"user_tz":240,"elapsed":319,"user":{"displayName":"Aleksander Ficek","userId":"04291360368766181612"}},"outputId":"d1649cbe-beec-4914-a18e-d18a3e8f4d15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mean_reward:9.25 +/- 0.89\n"]}]},{"cell_type":"code","source":["# Optional: Monitor training in tensorboard\n","%load_ext tensorboard\n","%tensorboard --logdir $tensorboard_log"],"metadata":{"id":"30s9TzD3iW_P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.learn(int(3e5), log_interval=20)"],"metadata":{"id":"lhkKTkvhiaiN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mean_reward, std_reward = evaluate_policy(model, model.get_env(), deterministic=True, n_eval_episodes=1)\n","\n","print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"],"metadata":{"id":"iu6HGjkDieFR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up fake display; otherwise rendering will fail\n","import os\n","os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n","os.environ['DISPLAY'] = ':1'\n","import base64\n","from pathlib import Path\n","from IPython import display as ipythondisplay\n","\n","def show_videos(video_path='', prefix=''):\n","  html = []\n","  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n","      video_b64 = base64.b64encode(mp4.read_bytes())\n","      html.append('''<video alt=\"{}\" autoplay \n","                    loop controls style=\"height: 400px;\">\n","                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n","                </video>'''.format(mp4, video_b64.decode('ascii')))\n","  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"],"metadata":{"id":"FbJpwY0ZvFeH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n","\n","def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n","  eval_env = DummyVecEnv([lambda: env_id])\n","  # Start the video at step=0 and record 500 steps\n","  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n","                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n","                              name_prefix=prefix)\n","\n","  obs = eval_env.reset()\n","  for _ in range(video_length):\n","    action, _ = model.predict(obs, deterministic=False)\n","    obs, _, _, _ = eval_env.step(action)\n","\n","  # Close the video recorder\n","  eval_env.close()"],"metadata":{"id":"gWkCTglEvgbc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# powers_step = [9.8, 12, 14, 16, 30, 50, 100, 150]\n","# powers_step = [1, 3, 5, 7, 9.8, 100, 150]\n","powers_step = [150, 100, 50, 40, 30, 16, 12]\n","\n","\n","\n","total_mean_rewards = []\n","\n","for each_power_step in powers_step:\n","  class CartPoleEnv(gym.Env):\n","      metadata = {\n","          'render.modes': ['human', 'rgb_array'],\n","          'video.frames_per_second' : 50\n","      }\n","\n","      def __init__(self):\n","          self.gravity = each_power_step\n","          self.masscart = 1.0\n","          self.masspole = 0.1\n","          self.total_mass = (self.masspole + self.masscart)\n","          self.length = 0.5 # actually half the pole's length\n","          self.polemass_length = (self.masspole * self.length)\n","          self.force_mag = 10.0\n","          self.tau = 0.02  # seconds between state updates\n","\n","          # Angle at which to fail the episode\n","          self.theta_threshold_radians = 12 * 2 * math.pi / 360\n","          self.x_threshold = 2.4\n","\n","          # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds\n","          high = np.array([\n","              self.x_threshold * 2,\n","              np.finfo(np.float32).max,\n","              self.theta_threshold_radians * 2,\n","              np.finfo(np.float32).max])\n","\n","          self.action_space = spaces.Discrete(2)\n","          self.observation_space = spaces.Box(-high, high)\n","\n","          self.seed()\n","          self.viewer = None\n","          self.state = None\n","\n","          self.steps_beyond_done = None\n","\n","      def seed(self, seed=None):\n","          self.np_random, seed = seeding.np_random(seed)\n","          return [seed]\n","\n","      def step(self, action):\n","          assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n","          state = self.state\n","          x, x_dot, theta, theta_dot = state\n","          force = self.force_mag if action==1 else -self.force_mag\n","          costheta = math.cos(theta)\n","          sintheta = math.sin(theta)\n","          temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass\n","          thetaacc = (self.gravity * sintheta - costheta* temp) / (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass))\n","          xacc  = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n","          x  = x + self.tau * x_dot\n","          x_dot = x_dot + self.tau * xacc\n","          theta = theta + self.tau * theta_dot\n","          theta_dot = theta_dot + self.tau * thetaacc\n","          self.state = (x,x_dot,theta,theta_dot)\n","          done =  x < -self.x_threshold \\\n","                  or x > self.x_threshold \\\n","                  or theta < -self.theta_threshold_radians \\\n","                  or theta > self.theta_threshold_radians\n","          done = bool(done)\n","\n","          if not done:\n","              reward = 1.0\n","          elif self.steps_beyond_done is None:\n","              # Pole just fell!\n","              self.steps_beyond_done = 0\n","              reward = 1.0\n","          else:\n","              if self.steps_beyond_done == 0:\n","                  logger.warn(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n","              self.steps_beyond_done += 1\n","              reward = 0.0\n","\n","          return np.array(self.state), reward, done, {}\n","\n","      def reset(self):\n","          self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n","          self.steps_beyond_done = None\n","          return np.array(self.state)\n","\n","      def render(self, mode='human'):\n","          screen_width = 600\n","          screen_height = 400\n","\n","          world_width = self.x_threshold*2\n","          scale = screen_width/world_width\n","          carty = 100 # TOP OF CART\n","          polewidth = 10.0\n","          polelen = scale * 1.0\n","          cartwidth = 50.0\n","          cartheight = 30.0\n","\n","          if self.viewer is None:\n","              from gym.envs.classic_control import rendering\n","              self.viewer = rendering.Viewer(screen_width, screen_height)\n","              l,r,t,b = -cartwidth/2, cartwidth/2, cartheight/2, -cartheight/2\n","              axleoffset =cartheight/4.0\n","              cart = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n","              self.carttrans = rendering.Transform()\n","              cart.add_attr(self.carttrans)\n","              self.viewer.add_geom(cart)\n","              l,r,t,b = -polewidth/2,polewidth/2,polelen-polewidth/2,-polewidth/2\n","              pole = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n","              pole.set_color(.8,.6,.4)\n","              self.poletrans = rendering.Transform(translation=(0, axleoffset))\n","              pole.add_attr(self.poletrans)\n","              pole.add_attr(self.carttrans)\n","              self.viewer.add_geom(pole)\n","              self.axle = rendering.make_circle(polewidth/2)\n","              self.axle.add_attr(self.poletrans)\n","              self.axle.add_attr(self.carttrans)\n","              self.axle.set_color(.5,.5,.8)\n","              self.viewer.add_geom(self.axle)\n","              self.track = rendering.Line((0,carty), (screen_width,carty))\n","              self.track.set_color(0,0,0)\n","              self.viewer.add_geom(self.track)\n","\n","          if self.state is None: return None\n","\n","          x = self.state\n","          cartx = x[0]*scale+screen_width/2.0 # MIDDLE OF CART\n","          self.carttrans.set_translation(cartx, carty)\n","          self.poletrans.set_rotation(-x[2])\n","\n","          return self.viewer.render(return_rgb_array = mode=='rgb_array')\n","\n","      def close(self):\n","          if self.viewer: self.viewer.close()\n","\n","\n","  mean_reward, std_reward = evaluate_policy(model, CartPoleEnv(), deterministic=True, n_eval_episodes=20)\n","\n","  total_mean_rewards.append(mean_reward)\n","  print(\"Gravity: \", each_power_step, \"Mean Reward: \", mean_reward, \"Std Deviation: \", std_reward)\n","  # print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n","\n","print(\"Step: \", powers_step)\n","print(\"Mean Rewards: \", total_mean_rewards)"],"metadata":{"id":"5SiNIVSpqUwO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = CartPoleEnv()\n","record_video(CartPoleEnv(), model, video_length=500, prefix='dqn-cartpole')\n","show_videos('videos', prefix='dqn-cartpole')"],"metadata":{"id":"lCt0Cgrkvk6L"},"execution_count":null,"outputs":[]}]}